{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define environment variables</b>\n",
    "\n",
    "To be used in future training steps.  Note that the BUCKET_NAME defined below must exist in the GCP project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BUCKET_NAME=ross-keras\n",
      "env: LOCAL_JOB_DIR=local-training-output\n",
      "env: JOB_NAME=keras_1101_job14\n",
      "env: REGION=us-central1\n",
      "env: MODEL_NAME=keras_wnd_model\n",
      "env: MODEL_VERSION=v1\n"
     ]
    }
   ],
   "source": [
    "%env BUCKET_NAME=ross-keras\n",
    "%env LOCAL_JOB_DIR=local-training-output\n",
    "%env JOB_NAME=keras_1101_job14\n",
    "%env REGION=us-central1\n",
    "%env MODEL_NAME=keras_wnd_model\n",
    "%env MODEL_VERSION=v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Perform training locally with default parameters</b>\n",
    "\n",
    "Training detail will be written locally to the folder referenced in the job-dir parameter.\n",
    "\n",
    "Note - creating the data will take some time as the MinMax normalizer needs to be fit over the 100 M plus training rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform local train \\\n",
    "  --package-path trainer \\\n",
    "  --module-name trainer.task \\\n",
    "  --job-dir $LOCAL_JOB_DIR \\\n",
    "  -- \\\n",
    "  --create-data=False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Perform training on AI Platform</b>\n",
    "\n",
    "The training job can also be run on AI Platform.  Note that in order for AI Platform to be able to complete the training job, the \"Google Cloud ML Engine Service Agent\" service account must be granted Cloud Storage and BigQuery admin roles.\n",
    "\n",
    "Important: A single training job (either locally or using AI Platform) must complete with the create-data flag set to true for the remainig functionality to compolete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --package-path trainer/ \\\n",
    "  --module-name trainer.task \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.5 \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --job-dir gs://${BUCKET_NAME}/keras-job-dir \\\n",
    "  -- \\\n",
    "  --create-data=False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Perform hyperparameter tuning on AI Platform</b>\n",
    "\n",
    "Training detail will be written to Cloud Storage in the folder referenced in the job-dir parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [keras_1101_job14_hpt] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe keras_1101_job14_hpt\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs keras_1101_job14_hpt\n",
      "jobId: keras_1101_job14_hpt\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training ${JOB_NAME}_hpt \\\n",
    "  --config hptuning_config.yaml \\\n",
    "  --package-path trainer/ \\\n",
    "  --module-name trainer.task \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.5 \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --job-dir gs://${BUCKET_NAME}/keras-job-dir-hpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Complete training on AI Platform</b>\n",
    "\n",
    "Now that hyperparameters have been tuned, perform deeper training with the optimal hyperparameters in place.  Note that we've increased the training size by explicitly setting the train-steps and num-epochs parameters in addition to the tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --package-path trainer/ \\\n",
    "  --module-name trainer.task \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.5 \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --job-dir gs://${BUCKET_NAME}/keras-job-dir \\\n",
    "  -- \\\n",
    "  --num-deep-layers=2 \\\n",
    "  --first-deep-layer-size=15 \\\n",
    "  --first-wide-layer-size=2262 \\\n",
    "  --learning-rate=0.005 \\\n",
    "  --wide-scale-factor=0.02 \\\n",
    "  --train-batch-size=12 \\\n",
    "  --train-steps=40 \\\n",
    "  --num-epochs=100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Host the trained model on AI Platform</b>\n",
    "\n",
    "Because we're passing a list of numpy arrays and not a single numpy array as input for inference, we'll need to establish a custom prediction module.  \n",
    "\n",
    "First, execute the setup script to create a distribution tarball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the tarball over to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp dist/trainer-0.1.tar.gz gs://${BUCKET_NAME}/staging-dir/trainer-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a new model on AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform models create $MODEL_NAME --regions $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create new version using our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai-platform versions create $MODEL_VERSION \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --python-version 3.5 \\\n",
    "  --origin gs://${BUCKET_NAME}/keras-job-dir \\\n",
    "  --package-uris gs://${BUCKET_NAME}/staging-dir/trainer-0.1.tar.gz \\\n",
    "  --prediction-class predictor.MyPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare a sample for inference</b>\n",
    "\n",
    "Note that we are using the same preprocessing methods used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Make an inference on a new sample.</b>\n",
    "\n",
    "Pass the sample object to the model hosted in AI Platform to return a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --version $MODEL_VERSION \\\n",
    "  --json-instances input_sample.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
