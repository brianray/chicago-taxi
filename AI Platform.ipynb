{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define environment variables</b>\n",
    "\n",
    "To be used in future training steps.  Note that the BUCKET_NAME defined below must exist in the GCP project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BUCKET_NAME=ross-keras\n",
      "env: LOCAL_JOB_DIR=local-training-output\n",
      "env: JOB_NAME=keras_1102_job4\n",
      "env: REGION=us-central1\n",
      "env: MODEL_NAME=keras_wnd_model\n",
      "env: MODEL_VERSION=v2\n"
     ]
    }
   ],
   "source": [
    "%env BUCKET_NAME=ross-keras\n",
    "%env LOCAL_JOB_DIR=local-training-output\n",
    "%env JOB_NAME=keras_1102_job4\n",
    "%env REGION=us-central1\n",
    "%env MODEL_NAME=keras_wnd_model\n",
    "%env MODEL_VERSION=v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Perform training locally with default parameters</b>\n",
    "\n",
    "Training detail will be written locally to the folder referenced in the job-dir parameter.\n",
    "\n",
    "Note - creating the data will take some time as the MinMax normalizer needs to be fit over the 100 M plus training rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform local train \\\n",
    "  --package-path trainer \\\n",
    "  --module-name trainer.task \\\n",
    "  --job-dir $LOCAL_JOB_DIR \\\n",
    "  -- \\\n",
    "  --create-data=False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Perform training on AI Platform</b>\n",
    "\n",
    "The training job can also be run on AI Platform.  Note that in order for AI Platform to be able to complete the training job, the \"Google Cloud ML Engine Service Agent\" service account must be granted Cloud Storage and BigQuery admin roles.\n",
    "\n",
    "Important: A single training job (either locally or using AI Platform) must complete with the create-data flag set to true for the remainig functionality to compolete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --package-path trainer/ \\\n",
    "  --module-name trainer.task \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.5 \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --job-dir gs://${BUCKET_NAME}/keras-job-dir-${JOB_NAME} \\\n",
    "  -- \\\n",
    "  --create-data=False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Perform hyperparameter tuning on AI Platform</b>\n",
    "\n",
    "Training detail will be written to Cloud Storage in the folder referenced in the job-dir parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [keras_1101_job15_hpt] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe keras_1101_job15_hpt\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs keras_1101_job15_hpt\n",
      "jobId: keras_1101_job15_hpt\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training ${JOB_NAME}_hpt \\\n",
    "  --config hptuning_config.yaml \\\n",
    "  --package-path trainer/ \\\n",
    "  --module-name trainer.task \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.5 \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --job-dir gs://${BUCKET_NAME}/keras-job-dir-hpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Complete training on AI Platform</b>\n",
    "\n",
    "Now that hyperparameters have been tuned, perform deeper training with the optimal hyperparameters in place.  Note that we've explicitly increased the train-steps and num-epochs parameters in addition to the tuned hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [keras_1102_job4] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe keras_1102_job4\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs keras_1102_job4\n",
      "jobId: keras_1102_job4\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --package-path trainer/ \\\n",
    "  --module-name trainer.task \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.5 \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --job-dir gs://${BUCKET_NAME}/keras-job-dir-${JOB_NAME} \\\n",
    "  -- \\\n",
    "  --num-deep-layers=2 \\\n",
    "  --first-deep-layer-size=30 \\\n",
    "  --first-wide-layer-size=1233 \\\n",
    "  --learning-rate=0.003 \\\n",
    "  --wide-scale-factor=0.094 \\\n",
    "  --train-batch-size=132 \\\n",
    "  --dropout-rate=0.4 \\\n",
    "  --train-steps=50000 \\\n",
    "  --num-epochs=100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Host the trained model on AI Platform</b>\n",
    "\n",
    "Because we're passing a list of numpy arrays and not a single numpy array as input for inference, we'll need to establish a custom prediction module.  \n",
    "\n",
    "First, execute the setup script to create a distribution tarball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "creating trainer.egg-info\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "making hard links in trainer-0.1...\n",
      "hard linking predictor.py -> trainer-0.1\n",
      "hard linking setup.py -> trainer-0.1\n",
      "hard linking trainer/__init__.py -> trainer-0.1/trainer\n",
      "hard linking trainer/create_data_func.py -> trainer-0.1/trainer\n",
      "hard linking trainer/create_scaler_func.py -> trainer-0.1/trainer\n",
      "hard linking trainer/model.py -> trainer-0.1/trainer\n",
      "hard linking trainer/task.py -> trainer-0.1/trainer\n",
      "hard linking trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "hard linking trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "hard linking trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "hard linking trainer.egg-info/requires.txt -> trainer-0.1/trainer.egg-info\n",
      "hard linking trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!python setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the tarball over to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  8.3 KiB/  8.3 KiB]                                                \n",
      "Operation completed over 1 objects/8.3 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp dist/trainer-0.1.tar.gz gs://${BUCKET_NAME}/staging-dir/trainer-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a new model on AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform models create $MODEL_NAME --regions $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create new version using our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating version (this might take a few minutes)......done.\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai-platform versions create $MODEL_VERSION \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --python-version 3.5 \\\n",
    "  --origin gs://${BUCKET_NAME}/keras-job-dir-${JOB_NAME} \\\n",
    "  --package-uris gs://${BUCKET_NAME}/staging-dir/trainer-0.1.tar.gz \\\n",
    "  --prediction-class predictor.MyPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare a sample for inference</b>\n",
    "\n",
    "Note that we are using the same preprocessing methods used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/rossclaytor/Desktop/taxi-git/chicago-taxi/venv/lib/python3.7/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "Produced sample with label 660 seconds.\n"
     ]
    }
   ],
   "source": [
    "!python create_sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Make an inference on a new sample.</b>\n",
    "\n",
    "Pass the sample object to the model hosted in AI Platform to return a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": \"640.1120370952192\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --version $MODEL_VERSION \\\n",
    "  --json-instances input_sample.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
